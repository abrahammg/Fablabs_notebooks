{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f0fa1ff",
   "metadata": {},
   "source": [
    "# GAN for Generating Human Faces (CelebA)\n",
    "\n",
    "In this notebook, we will build and train a simple Generative Adversarial Network (GAN) to generate human face images based on the CelebA dataset. Each step includes detailed explanations to help you understand how the model works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3999f77c",
   "metadata": {},
   "source": [
    "## 1. Imports and Hyperparameters\n",
    "\n",
    "We import necessary libraries and define hyperparameters:\n",
    "\n",
    "- `tensorflow` and `keras` for model building and training.\n",
    "- `tensorflow_datasets` to load CelebA.\n",
    "- `matplotlib` and `numpy` for visualization and numerical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "711ecbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 18:05:31.416416: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-12 18:05:31.417512: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-12 18:05:31.423914: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-12 18:05:31.437566: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749744331.457379  291965 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749744331.463141  291965 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749744331.481122  291965 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749744331.481138  291965 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749744331.481139  291965 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749744331.481140  291965 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-12 18:05:31.487048: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers, models\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtfds\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 100        # Dimension of the random noise vector\n",
    "img_height = 64         # Target image height\n",
    "img_width = 64          # Target image width\n",
    "channels = 3            # RGB channels\n",
    "img_shape = (img_height, img_width, channels)\n",
    "batch_size = 128        # Number of images per batch\n",
    "epochs = 5000           # Total training steps (batches)\n",
    "sample_interval = 500   # Interval to generate sample images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f282ee71",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess the CelebA Dataset\n",
    "\n",
    "We load CelebA via `tensorflow_datasets`, resize images to 64×64, and normalize pixel values to the range [-1, 1] to match the `tanh` output of the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84674934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sample):\n",
    "    # Convert uint8 image to float32 in [-1,1]\n",
    "    image = tf.image.resize(sample['image'], [img_height, img_width])\n",
    "    image = (tf.cast(image, tf.float32) - 127.5) / 127.5\n",
    "    return image\n",
    "\n",
    "# Load dataset\n",
    "ds = tfds.load('celeb_a', split='train', shuffle_files=True)\n",
    "ds = ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds = ds.shuffle(10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "print('Dataset ready:', ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4a2df3",
   "metadata": {},
   "source": [
    "## 3. Build the Generator Model\n",
    "\n",
    "The generator will transform a random noise vector into a 64×64×3 image. We use:\n",
    "\n",
    "- `Dense` and `Reshape` to project noise into a feature map.\n",
    "- `BatchNormalization` and `LeakyReLU` for stable training.\n",
    "- `Conv2DTranspose` layers to upsample to 64×64.\n",
    "- `tanh` activation to output images in [-1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8107773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    model = models.Sequential(name='Generator')\n",
    "    # project and reshape\n",
    "    model.add(layers.Dense(8*8*256, input_dim=latent_dim))\n",
    "    model.add(layers.Reshape((8, 8, 256)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "\n",
    "    # upsample to 16x16\n",
    "    model.add(layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "\n",
    "    # upsample to 32x32\n",
    "    model.add(layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "\n",
    "    # upsample to 64x64\n",
    "    model.add(layers.Conv2DTranspose(channels, kernel_size=4, strides=2, padding='same',\n",
    "                                     activation='tanh'))\n",
    "    return model\n",
    "\n",
    "generator = build_generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ce5c23",
   "metadata": {},
   "source": [
    "## 4. Build the Discriminator Model\n",
    "\n",
    "The discriminator is a binary classifier that distinguishes real from generated faces. We use:\n",
    "\n",
    "- `Conv2D` layers to extract features and downsample.\n",
    "- `LeakyReLU` for non-linearity.\n",
    "- `Dropout` to regularize.\n",
    "- `Dense` with `sigmoid` for final probability output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d690b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    model = models.Sequential(name='Discriminator')\n",
    "    model.add(layers.Conv2D(64, kernel_size=4, strides=2, padding='same',\n",
    "                             input_shape=img_shape))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(256, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "discriminator = build_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31da5b9d",
   "metadata": {},
   "source": [
    "## 5. Compile the Discriminator and GAN\n",
    "\n",
    "Compile discriminator with binary crossentropy and Adam optimizer. Then build the GAN by connecting generator and discriminator, freezing discriminator weights for generator training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c29eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.compile(optimizer=tf.keras.optimizers.Adam(0.0002, 0.5),\n",
    "                       loss='binary_crossentropy',\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "z = layers.Input(shape=(latent_dim,))\n",
    "img = generator(z)\n",
    "discriminator.trainable = False\n",
    "validity = discriminator(img)\n",
    "gan = models.Model(z, validity)\n",
    "gan.compile(optimizer=tf.keras.optimizers.Adam(0.0002, 0.5),\n",
    "            loss='binary_crossentropy')\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93d503b",
   "metadata": {},
   "source": [
    "## 6. Training the GAN\n",
    "\n",
    "We train with alternating steps:\n",
    "1. Train discriminator on real and fake images.\n",
    "2. Train generator via GAN with labels as real to fool discriminator.\n",
    "We periodically sample generated faces to visualize progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28128f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def sample_images(step, n=5):\n",
    "    noise = np.random.normal(0, 1, (n, latent_dim))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5  # scale to [0,1]\n",
    "    fig, axes = plt.subplots(1, n, figsize=(n*2,2))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(gen_imgs[i])\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def train(epochs, batch_size=128):\n",
    "    half = batch_size // 2\n",
    "    for step in range(epochs):\n",
    "        # Train Discriminator\n",
    "        imgs = next(iter(ds))  # real images batch\n",
    "        noise = np.random.normal(0, 1, (half, latent_dim))\n",
    "        fake = generator.predict(noise)\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, np.ones((half,1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(fake, np.zeros((half,1)))\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Train Generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        g_loss = gan.train_on_batch(noise, np.ones((batch_size,1)))\n",
    "\n",
    "        # Display progress\n",
    "        if step % sample_interval == 0:\n",
    "            print(f\"Step {step} [D loss: {d_loss[0]:.4f}] [G loss: {g_loss:.4f}]\")\n",
    "            sample_images(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28934b5",
   "metadata": {},
   "source": [
    "## 7. Run Training\n",
    "\n",
    "Start training. Be patient, as generating high-quality faces takes time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39edaea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64279968",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "- We built a GAN on CelebA to generate 64×64 RGB faces.\n",
    "- Each step was explained, from data loading to training loop.\n",
    "- Next, try increasing model depth, adding residual blocks, or using data augmentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
